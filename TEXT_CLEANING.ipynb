{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Whitespace\n",
        "It refers to spaces, tabs, or newlines (\\n, \\t) used to separate words or format text."
      ],
      "metadata": {
        "id": "4n-mcEx7TtpN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Punctuation**\n",
        "\n",
        "Definition: Punctuation marks are symbols used in text for grammar, emphasis, or structure.\n",
        "Examples: . , ? ! ; : ( ) \" ' -\n",
        "\n",
        "Why they matter:\n",
        "\n",
        "Sometimes punctuation is noise (e.g., commas in a paragraph).\n",
        "\n",
        "Sometimes punctuation carries meaning:\n",
        "\n",
        "  “Let’s eat, Grandma!” vs “Let’s eat Grandma!”\n",
        "\n",
        "  Sentiment: “I am happy!!!” vs “I am happy.”\n",
        "\n",
        "Handling:\n",
        "\n",
        "1. Often removed for topic modeling or classification.\n",
        "\n",
        "2. Usually kept for sentiment analysis, QA, and modern transformer models."
      ],
      "metadata": {
        "id": "jEVr6TyMS81C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stopwords**\n",
        "\n",
        "Definition: Stopwords are very common words in a language that usually don’t add much meaning to the text.\n",
        "Example in English: the, is, in, at, and, of, on, a, to\n",
        "\n",
        "Why they matter:\n",
        "\n",
        "In classical NLP, stopwords are often removed to reduce noise and vocabulary size.\n",
        "But in modern NLP (transformers), removing stopwords is not always recommended because context matters."
      ],
      "metadata": {
        "id": "LkDHqcHwS6RG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **How much cleaning is needed ?**\n",
        "\n",
        "Classical ML (Naive Bayes, SVM, Logistic Regression, etc.):\n",
        "Heavy cleaning (stopword removal, stemming, lemmatization) usually improves accuracy.\n",
        "\n",
        "Deep learning (RNNs, LSTMs, CNNs):\n",
        "Moderate cleaning (lowercasing, punctuation removal, normalization) is helpful, but too much cleaning (like removing stopwords) might remove useful context.\n",
        "\n",
        "Transformers (BERT, GPT, T5, etc.):\n",
        "Minimal cleaning is enough, since tokenizers handle punctuation, casing, and subword segmentation. Over-cleaning (e.g., removing stopwords) can actually hurt performance."
      ],
      "metadata": {
        "id": "JQ8ZB25uSHnJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLEANING TEXT MANUALLY"
      ],
      "metadata": {
        "id": "1vPHm4R_UXVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"/content/metamorphosis_clean.txt\"\n",
        "file = open(filename, 'rt')\n",
        "text = file.read()\n",
        "file.close()\n"
      ],
      "metadata": {
        "id": "6kA68IhIIeWx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split by whitespace\n",
        "words = text.split()\n",
        "print(words[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDu4XIn3ItDM",
        "outputId": "ff926e97-ed81-4098-8a77-a3438a71f0ca"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['One', 'morning,', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams,', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin.', 'He', 'lay', 'on', 'his', 'armour-like', 'back,', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly,', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections.', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment.', 'His', 'many', 'legs,', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him,', 'waved', 'about', 'helplessly', 'as', 'he', 'looked.', '“What’s', 'happened', 'to', 'me?”', 'he', 'thought.', 'It', 'wasn’t', 'a', 'dream.', 'His', 'room,', 'a', 'proper', 'human']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# regex method\n",
        "import re\n",
        "words = re.split(r'[-\\s.,;!?]+', text)\n",
        "print(words[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uO_YDtvVI4aV",
        "outputId": "310f0755-75fe-47a5-b910-cea8adf72664"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', '“What’s', 'happened', 'to', 'me', '”', 'he', 'thought', 'It', 'wasn’t', 'a', 'dream', 'His', 'room', 'a']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split by whitespace and remove punctuation\n",
        "import string\n",
        "import re\n",
        "filename = \"/content/metamorphosis_clean.txt\"\n",
        "file = open(filename, 'rt')\n",
        "text = file.read()\n",
        "file.close()\n",
        "words = text.split()\n",
        "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "stripped = [re_punc.sub('', w) for w in words]\n",
        "print(stripped[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VniDaU3jJ0Ga",
        "outputId": "f6cd4f88-eec5-4112-d1b2-b27e81eb072b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armourlike', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', '“What’s', 'happened', 'to', 'me”', 'he', 'thought', 'It', 'wasn’t', 'a', 'dream', 'His', 'room', 'a', 'proper', 'human']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# normalizing the case\n",
        "filename = \"/content/metamorphosis_clean.txt\"\n",
        "file = open(filename, 'rt')\n",
        "text = file.read()\n",
        "file.close()\n",
        "words = text.split()\n",
        "words = [word.lower() for word in words]\n",
        "print(words[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqX2_4lNLO6N",
        "outputId": "fa173e0e-74c0-482e-ba9c-71d6fb480f66"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['one', 'morning,', 'when', 'gregor', 'samsa', 'woke', 'from', 'troubled', 'dreams,', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin.', 'he', 'lay', 'on', 'his', 'armour-like', 'back,', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly,', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections.', 'the', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment.', 'his', 'many', 'legs,', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him,', 'waved', 'about', 'helplessly', 'as', 'he', 'looked.', '“what’s', 'happened', 'to', 'me?”', 'he', 'thought.', 'it', 'wasn’t', 'a', 'dream.', 'his', 'room,', 'a', 'proper', 'human']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLEANING TEXT USING NLTK"
      ],
      "metadata": {
        "id": "c3YAUiqXUeoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJ34aereL_pJ",
        "outputId": "0d43c7f0-215c-4243-d327-46bf490e845a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")   # <-- new requirement\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxE58KCONuIg",
        "outputId": "fa30a020-e19a-434f-8ab1-d31bb82ddc19"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split into sentences using nltk\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "filename = \"/content/metamorphosis_clean.txt\"\n",
        "with open(filename, \"rt\") as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Sentence tokenization\n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences[0])  # prints the first sentence\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2sxARW0MElX",
        "outputId": "36f56624-faf2-4b1e-f67b-7aab42b23f1f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One morning, when Gregor Samsa woke from troubled dreams, he found\n",
            "himself transformed in his bed into a horrible vermin.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split into words using nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "filename = \"/content/metamorphosis_clean.txt\"\n",
        "with open(filename, \"rt\") as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Word tokenization\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0oYO-0kN_Ki",
        "outputId": "303ef352-2c4e-4f07-e7a0-ac6be933537a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['One', 'morning', ',', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', ',', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', '.', 'He', 'lay', 'on', 'his', 'armour-like', 'back', ',', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', ',', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', '.', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', '.', 'His', 'many', 'legs', ',', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', ',', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', '.', '“', 'What', '’', 's', 'happened']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# filtering out punctuation using nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "filename = \"/content/metamorphosis_clean.txt\"\n",
        "with open(filename, \"rt\") as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Word tokenization\n",
        "tokens = word_tokenize(text)\n",
        "words = [word for word in tokens if word.isalpha()]\n",
        "print(words[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eNLNn7AORYm",
        "outputId": "7d03cac9-580e-4a41-cf66-9f626db212be"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'What', 's', 'happened', 'to', 'me', 'he', 'thought', 'It', 'wasn', 't', 'a', 'dream', 'His', 'room', 'a', 'proper']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# filtering stopwords using nltk\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Get English stopwords\n",
        "stop_words = stopwords.words(\"english\")\n",
        "print(stop_words[:20])  # show first 20 stopwords\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHP0nhC9PA8E",
        "outputId": "b49696f0-22ed-4053-dfdb-30578bbc6662"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}