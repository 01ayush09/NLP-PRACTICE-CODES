{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d389c250-9137-4d82-aaf9-e3e1aa51ca2f",
   "metadata": {},
   "source": [
    "# Punctuation and Stopwords\n",
    "Punctuation consists of symbols used for sentence structure, and they are usually removed to reduce noise unless needed for sentiment or syntax analysis.\n",
    "\n",
    "example - . , ! ? : ; \" ' ( ) - ...\n",
    "\n",
    "Stopwords are high-frequency function words like \"the\", \"is\", \"and\" that often carry little semantic meaning and are removed to reduce dimensionality in NLP tasks.\n",
    "\n",
    "example - the, is, am, are, was, were, in, on, at, and, of, to\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3d244a-f8ef-4a4c-93e5-72872f9cee90",
   "metadata": {},
   "source": [
    "# Morphology VS Syntax VS Semantics\n",
    "\n",
    "Morphology studies the internal structure of words — how words are formed from smaller meaningful units called morphemes.\n",
    "A morpheme is the smallest unit of meaning.\n",
    "\n",
    "Syntax studies how words combine to form grammatically correct sentences.\n",
    "It focuses on sentence structure.\n",
    "\n",
    "Semantics studies the meaning of words, phrases, and sentences independent of context.\n",
    "\n",
    "Example - \"The dogs were running.\"\n",
    "\n",
    "Morphology :\n",
    "    \n",
    "    dogs → dog + s\n",
    "    running → run + ing\n",
    "\n",
    "Syntax :\n",
    "\n",
    "     dogs = subject\n",
    "     were running = verb phrase\n",
    "\n",
    "Semantics :\n",
    "\n",
    "    An agent → dogs\n",
    "    An event → running\n",
    "    A time frame → past ongoing\n",
    "    Meaning: There exist dogs who were in the state of running              in the past.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d9484f-6f02-4494-b7cd-aaec7192a693",
   "metadata": {},
   "source": [
    "# BOW \n",
    "\n",
    "Bag of Words is a simple text representation method where we ignore grammar, word order and we only count word frequency.\n",
    "\n",
    "It converts text → numeric vector.\n",
    "\n",
    "# Reasons not to use BOW are -\n",
    "1. No Importance Weighting as common words dominate.\n",
    "2. Completely Ignores Document-Level Information as BoW only counts word frequency in a document.\n",
    "3. No context awareness.\n",
    "\n",
    "# TF-IDF\n",
    "\n",
    "TF-IDF improves BoW by giving high weight to important words and giving low weight to common words.\n",
    "\n",
    "TF(Term Frequency) Measures how frequently a term appears in a document.\n",
    "\n",
    "    TF(t,d) = (count of term t in document d) / (total number of terms in document d)\n",
    "\n",
    "IDF (Inverse Document Frequency) Measures how rare a word is across all documents.\n",
    "\n",
    "    IDF(t) = log( N / df(t) )\n",
    "\n",
    "    Where:\n",
    "    N = total number of documents\n",
    "    df(t) = number of documents containing term t\n",
    "\n",
    "\n",
    "# Reasons not to use TF-IDF are - \n",
    "1. No Context Awareness (Major Limitation) - TF-IDF treats words independently.\n",
    "2. Extremely Sparse & High Dimensional - Vocabulary size can easily reach very large.\n",
    "3. TF-IDF cannot handle unseen words well.\n",
    "\n",
    "\n",
    "\n",
    "# n-gram\n",
    "An n-gram is a contiguous sequence of n tokens from a text, used to capture local word order information in classical NLP models.\n",
    "\n",
    "example - \"I love natural language processing\"\n",
    "\n",
    "for n = 1 (unigram)\n",
    "    [\"I\", \"love\", \"natural\", \"language\", \"processing\"]\n",
    "\n",
    "for n = 2 (bigram)\n",
    "    [\"I love\", \"love natural\", \"natural language\", \"language processing\"]\n",
    "\n",
    "similarly upto n.\n",
    "\n",
    "# Reasons not to use n-gram are \n",
    "1. Feature Explosion - If vocabulary size = 50,000 then Possible bigrams:50000 x 50000 =2.5 billion. so huge memory issue.\n",
    "2. Many n-grams appear rarely → unreliable statistics.\n",
    "3. Still No Long-Range Context awareness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cb0661-50c3-4b38-aec2-f79b551333c4",
   "metadata": {},
   "source": [
    "# Semantic Similarity \n",
    "\n",
    "Semantic similarity measures how similar two pieces of text are in meaning — not just in words.\n",
    "\n",
    "Various computational approaches in NLP are - \n",
    "\n",
    "1. Cosine Similarity + TF-IDF (Classical Approach)\n",
    "\n",
    "       Step 1: Convert sentences into TF-IDF vectors\n",
    "       Step 2: Compute cosine similarity\n",
    "\n",
    "       Cosine Similarity Formula\n",
    "\n",
    "             # Cosine(A, B) = (A · B) / (||A|| * ||B||)\n",
    "\n",
    "       Where:\n",
    "             A⋅B = dot product\n",
    "             ∥A∥ = vector magnitude\n",
    "\n",
    "       Range:\n",
    "              1 → identical\n",
    "              0 → unrelated\n",
    "             -1 → opposite (rare in TF-IDF)\n",
    "\n",
    "\n",
    "2. Embedding-Based Approaches - Words are mapped to dense vectors.Then compute cosine similarity between vectors.Used in models like Word2Vec, GloVe, FastText.\n",
    "\n",
    "\n",
    "4. Transformer-Based Semantic Similarity - first Encode sentence into dense vector and then compare vectors using cosine similarity.Used in models like - BERT , Transformers encoders.\n",
    "\n",
    "\n",
    "5. Sentence-Level Similarity Pipeline (Modern Production)\n",
    "\n",
    "   > Convert sentence → embedding (768-d vector)\n",
    "\n",
    "   > Store embeddings in vector database\n",
    "\n",
    "   > Use cosine similarity or dot product\n",
    "\n",
    "   > Retrieve top-k similar sentences\n",
    "\n",
    "   and this is used in semantic search, RAG pipelines and Recommendation systems.\n",
    "\n",
    "# Advanced Interview Insight \n",
    "\n",
    "Cosine similarity works because It measures angle between vectors, ignoring magnitude. But, In large-scale systems dot product is often used for efficiency.\n",
    "\n",
    "\n",
    "# POS Tagging\n",
    "\n",
    "POS (Part-of-Speech) tagging is the process of assigning a grammatical category to each word in a sentence. and it is used in NER, MAchine Translation etc.\n",
    "\n",
    "And it actually answers: What role is each word playing in the sentence?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c295935a-445b-467e-8302-e8a267e4fce3",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "Lemmatization is the process of reducing words to their dictionary base form using vocabulary and morphological analysis, often considering part-of-speech to ensure linguistically correct outputs.\n",
    "\n",
    "Wrorking Steps are - \n",
    "\n",
    "1️⃣ Identify the word\n",
    "2️⃣ Determine its POS (noun, verb, adj, etc.)\n",
    "3️⃣ Apply morphological rules\n",
    "4️⃣ Return dictionary base form\n",
    "\n",
    "# Stemming \n",
    "\n",
    "Stemming is a rule-based process of reducing words to their root form by removing suffixes, without using a dictionary or morphological analysis.\n",
    "\n",
    "Example - \"The children are running faster.\"\n",
    "\n",
    "| Word     | Stemming | Lemmatization |\n",
    "| -------- | -------- | ------------- |\n",
    "| children | children | child         |\n",
    "| running  | run      | run           |\n",
    "| faster   | faster   | fast          |\n",
    "\n",
    "Conceptual difference - \n",
    "\n",
    "| Stemming              | Lemmatization            |\n",
    "| --------------------- | ------------------------ |\n",
    "| Rule-based truncation | Linguistic normalization |\n",
    "| No dictionary         | Uses dictionary          |\n",
    "| Fast but rough        | Slower but accurate      |\n",
    "| May produce non-words | Produces valid words     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415428fc-05a9-4755-a26d-b0c87946a02b",
   "metadata": {},
   "source": [
    "# Implementing n-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "414e937d-bcdf-46c7-9a7c-52e23df223ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams: ['i', 'love', 'natural', 'language', 'processing']\n",
      "Bigrams: ['i love', 'love natural', 'natural language', 'language processing']\n",
      "Trigrams: ['i love natural', 'love natural language', 'natural language processing']\n"
     ]
    }
   ],
   "source": [
    "# n-gram \n",
    "\n",
    "def generate_ngrams(text, n):\n",
    "    tokens = text.lower().split()\n",
    "    ngrams = []\n",
    "\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngram = \" \".join(tokens[i:i+n])\n",
    "        ngrams.append(ngram)\n",
    "\n",
    "    return ngrams\n",
    "\n",
    "\n",
    "text = \"I love natural language processing\"\n",
    "\n",
    "print(\"Unigrams:\", generate_ngrams(text, 1))\n",
    "print(\"Bigrams:\", generate_ngrams(text, 2))\n",
    "print(\"Trigrams:\", generate_ngrams(text, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef56be60-2dd0-4e03-addd-3f712f65a766",
   "metadata": {},
   "source": [
    "# Write a python function that takes document as input and returns a dictionary containing the frequency of each word in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "260fc71c-ee63-453c-8183-cd98c2612c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nlp': 2, 'is': 2, 'fun': 1, 'powerful': 1}\n"
     ]
    }
   ],
   "source": [
    "# simple implementation\n",
    "\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def word_frequency(document: str) -> dict:\n",
    "    \"\"\"\n",
    "    Takes a document string and returns a dictionary\n",
    "    containing frequency of each word.\n",
    "    \"\"\"\n",
    "    if not document:\n",
    "        return {}\n",
    "\n",
    "    # Normalize text: lowercase + remove punctuation\n",
    "    tokens = re.findall(r'\\b\\w+\\b', document.lower())\n",
    "\n",
    "    freq = defaultdict(int)\n",
    "    for word in tokens:\n",
    "        freq[word] += 1\n",
    "\n",
    "    return dict(freq)\n",
    "\n",
    "doc = \"NLP is fun. NLP is powerful!\"\n",
    "print(word_frequency(doc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a73b168-5b6b-4ce3-9286-a1df03e2eda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nlp': 2, 'is': 2, 'fun': 1, 'powerful': 1}\n"
     ]
    }
   ],
   "source": [
    "# implementation using collections.Counter\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def word_frequency(document: str) -> dict:\n",
    "    tokens = re.findall(r'\\b\\w+\\b', document.lower())\n",
    "    return dict(Counter(tokens))\n",
    "\n",
    "doc = \"NLP is fun. NLP is powerful!\"\n",
    "print(word_frequency(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd74f32d-9a4b-486e-b630-a1827e8bf846",
   "metadata": {},
   "source": [
    "# Create a function to clean and tokenize given text , removing punctuation and converting words into lowercase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ca82f51-ae55-4f4f-981e-4875619142e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'nlp', 'is', 'amazing']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_and_tokenize(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Cleans and tokenizes input text by:\n",
    "    - Converting to lowercase\n",
    "    - Removing punctuation\n",
    "    - Splitting into words\n",
    "    \n",
    "    Returns:\n",
    "        List of tokens\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation and extract words\n",
    "    tokens = re.findall(r'\\b[a-z0-9]+\\b', text)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "text = \"Hello, World! NLP is AMAZING!!!\"\n",
    "print(clean_and_tokenize(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3360b719-eafa-40d5-b20e-fbe6c5f80e5e",
   "metadata": {},
   "source": [
    "# Develop a function to remove stopwords from given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e033cd4-9973-4f23-b27a-c0ff2eb4b163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['simple', 'example', 'removing', 'stopwords', 'text']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "DEFAULT_STOPWORDS = {\n",
    "    \"a\", \"an\", \"the\", \"is\", \"are\", \"was\", \"were\",\n",
    "    \"in\", \"on\", \"at\", \"to\", \"for\", \"of\", \"and\",\n",
    "    \"or\", \"but\", \"if\", \"then\", \"this\", \"that\",\n",
    "    \"it\", \"as\", \"with\", \"by\", \"from\"\n",
    "}\n",
    "\n",
    "def remove_stopwords(text: str, stopwords: set = None) -> list:\n",
    "    \"\"\"\n",
    "    Removes stopwords from input text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        stopwords (set): Set of stopwords (optional)\n",
    "        \n",
    "    Returns:\n",
    "        List of filtered tokens\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "\n",
    "    if stopwords is None:\n",
    "        stopwords = DEFAULT_STOPWORDS\n",
    "\n",
    "    # Lowercase + tokenize\n",
    "    tokens = re.findall(r'\\b[a-z0-9]+\\b', text.lower())\n",
    "\n",
    "    # Remove stopwords\n",
    "    filtered_tokens = [word for word in tokens if word not in stopwords]\n",
    "\n",
    "    return filtered_tokens\n",
    "\n",
    "text = \"This is a simple example of removing stopwords from text.\"\n",
    "print(remove_stopwords(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b0c7834-6381-474b-959f-afe4bea8d6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['simple', 'example', 'removing', 'stopwords', 'text']\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords_text(text: str, stopwords: set = None) -> str:\n",
    "    tokens = remove_stopwords(text, stopwords)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "text = \"This is a simple example of removing stopwords from text.\"\n",
    "print(remove_stopwords(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2aced5-1e9b-4db7-ad5e-8a2573d899ba",
   "metadata": {},
   "source": [
    "# Lemmatization using POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d2c37ff-4480-4866-ab97-4387ac013837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\ayush\\.conda\\envs\\machinelearning\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\ayush\\.conda\\envs\\machinelearning\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\ayush\\.conda\\envs\\machinelearning\\lib\\site-packages (from nltk) (1.5.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ayush\\.conda\\envs\\machinelearning\\lib\\site-packages (from nltk) (2026.1.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ayush\\.conda\\envs\\machinelearning\\lib\\site-packages (from nltk) (4.67.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\ayush\\.conda\\envs\\machinelearning\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18ae42c0-c6bd-495a-a984-50401c6ab869",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ayush/nltk_data'\n    - 'C:\\\\Users\\\\ayush\\\\.conda\\\\envs\\\\machinelearning\\\\nltk_data'\n    - 'C:\\\\Users\\\\ayush\\\\.conda\\\\envs\\\\machinelearning\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\ayush\\\\.conda\\\\envs\\\\machinelearning\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ayush\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[0;32m      5\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe children are running towards a better place.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m tokens \u001b[38;5;241m=\u001b[39m word_tokenize(sentence)\n\u001b[0;32m      7\u001b[0m tagged_tokens \u001b[38;5;241m=\u001b[39m pos_tag(tokens)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_wordnet_pos\u001b[39m(tag):\n",
      "File \u001b[1;32m~\\.conda\\envs\\machinelearning\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32m~\\.conda\\envs\\machinelearning\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\.conda\\envs\\machinelearning\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[1;32m~\\.conda\\envs\\machinelearning\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[1;32m~\\.conda\\envs\\machinelearning\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32m~\\.conda\\envs\\machinelearning\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ayush/nltk_data'\n    - 'C:\\\\Users\\\\ayush\\\\.conda\\\\envs\\\\machinelearning\\\\nltk_data'\n    - 'C:\\\\Users\\\\ayush\\\\.conda\\\\envs\\\\machinelearning\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\ayush\\\\.conda\\\\envs\\\\machinelearning\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ayush\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sentence = \"The children are running towards a better place.\"\n",
    "tokens = word_tokenize(sentence)\n",
    "tagged_tokens = pos_tag(tokens)\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    "    elif tag.startswith('V'):\n",
    "        return 'v'\n",
    "    elif tag.startswith('N'):\n",
    "        return 'n'\n",
    "    elif tag.startswith('R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return 'n'\n",
    "\n",
    "\n",
    "lemmatized_sentence = []\n",
    "for word, tag in tagged_tokens:\n",
    "    if word.lower() == 'are' or word.lower() in ['is', 'am']:\n",
    "        lemmatized_sentence.append(word)\n",
    "    else:\n",
    "        lemmatized_sentence.append(\n",
    "            lemmatizer.lemmatize(word, get_wordnet_pos(tag)))\n",
    "print(\"Original Sentence: \", sentence)\n",
    "print(\"Lemmatized Sentence: \", ' '.join(lemmatized_sentence))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93e96e6-4548-4e9a-a9cb-b0a09307dabf",
   "metadata": {},
   "source": [
    "# STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44a49f2f-b0b2-47ad-80f9-d745e780b327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'cat', 'were', 'runn', 'and', 'studi', 'were', 'complet']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def simple_stemmer(word: str) -> str:\n",
    "    \"\"\"\n",
    "    Very basic rule-based stemmer.\n",
    "    Not full Porter algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    word = word.lower()\n",
    "\n",
    "    # Step 1: plurals\n",
    "    if word.endswith(\"ies\"):\n",
    "        return word[:-3] + \"i\"\n",
    "    elif word.endswith(\"s\") and len(word) > 3:\n",
    "        return word[:-1]\n",
    "\n",
    "    # Step 2: common verb endings\n",
    "    if word.endswith(\"ing\") and len(word) > 4:\n",
    "        return word[:-3]\n",
    "    elif word.endswith(\"ed\") and len(word) > 3:\n",
    "        return word[:-2]\n",
    "\n",
    "    return word\n",
    "\n",
    "\n",
    "def stem_text(text: str) -> list:\n",
    "    tokens = re.findall(r'\\b[a-z]+\\b', text.lower())\n",
    "    return [simple_stemmer(word) for word in tokens]\n",
    "\n",
    "text = \"The cats were running and studies were completed\"\n",
    "print(stem_text(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f88599b5-fde7-4b54-80e0-3d8550c5b65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'cat', 'were', 'run', 'faster', 'than', 'the', 'other', 'runner']\n"
     ]
    }
   ],
   "source": [
    "# using NLTK library \n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def stem_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = text.lower().split()   # no punkt dependency\n",
    "    stems = [stemmer.stem(word) for word in tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "# Example\n",
    "text = \"The cats were running faster than the other runners\"\n",
    "print(stem_text(text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161c9a7f-378f-4c23-b1b3-24493f966d24",
   "metadata": {},
   "source": [
    "# Design a python function that calculates cosine similarity between two text documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a860aa28-9c7d-4d7c-bcf4-a999fa95de91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.8333333333333335\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def cosine_similarity(doc1, doc2):\n",
    "    # 1️⃣ Tokenization (simple)\n",
    "    tokens1 = doc1.lower().split()\n",
    "    tokens2 = doc2.lower().split()\n",
    "    \n",
    "    # 2️⃣ Build vocabulary\n",
    "    vocabulary = set(tokens1).union(set(tokens2))\n",
    "    \n",
    "    # 3️⃣ Create frequency vectors\n",
    "    freq1 = Counter(tokens1)\n",
    "    freq2 = Counter(tokens2)\n",
    "    \n",
    "    vector1 = [freq1[word] for word in vocabulary]\n",
    "    vector2 = [freq2[word] for word in vocabulary]\n",
    "    \n",
    "    # 4️⃣ Compute dot product\n",
    "    dot_product = sum(v1 * v2 for v1, v2 in zip(vector1, vector2))\n",
    "    \n",
    "    # 5️⃣ Compute magnitudes\n",
    "    magnitude1 = math.sqrt(sum(v ** 2 for v in vector1))\n",
    "    magnitude2 = math.sqrt(sum(v ** 2 for v in vector2))\n",
    "    \n",
    "    # 6️⃣ Avoid division by zero\n",
    "    if magnitude1 == 0 or magnitude2 == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # 7️⃣ Cosine similarity\n",
    "    return dot_product / (magnitude1 * magnitude2)\n",
    "\n",
    "\n",
    "# ✅ Example\n",
    "doc1 = \"I love NLP and machine learning\"\n",
    "doc2 = \"I love deep learning and NLP\"\n",
    "\n",
    "similarity = cosine_similarity(doc1, doc2)\n",
    "print(\"Cosine Similarity:\", similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f291525f-50e0-4464-ac6b-96f381d4648c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
